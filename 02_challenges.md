# Challenges

- Themes:
    1. Volume of data
    2. Software environment and documentation
- Word count: **2,000**
- Authors: All

---

Since the LHC began delivering collisions in 2010 and as of writing this document (March 2016), the four detectors have recorded a total volume of [INSERT NUMBER HERE] of data. It is important to note that these data represent a tiny fraction (0.000000000??%) of the total data *generated* by the LHC. Performing an analysis on this volume of data is far from trivial. First, the data from the collisions, which are in the form of electrical signals from individual channels within each detector, must be pieced together or reconstructed to form a coherent image of the collision itself. Then, analysis software must sift through trillions upon trillions of collisions to look for signatures associated with physics phenomena.

The LHC collaborations rely on the Worldwide LHC Computing Grid (or the Grid, for short) to perform these analysis tasks. The Grid is a network of interconnected computers distributed all over the globe, and datasets are broken into chunks to facilitate processing.