## Volume of data

Since the LHC began delivering collisions in 2010, and as of writing this document (June 2016), the four detectors have recorded several tens of petabytes (PB) of collision data. Performing an analysis on this volume of data involves several stages. First, the data from the collisions, which are in the form of electrical signals from individual channels within each detector, must be pieced together or reconstructed to form a coherent image of the collision itself. Then, analysis software must sift through the collisions to look for signatures associated with predicted physics phenomena.

The LHC collaborations rely on the Worldwide LHC Computing Grid (or the Grid, for short) \footnote{\url{http://wlcg.web.cern.ch/}} to perform these analysis tasks. The Grid is a network of interconnected computers distributed all over the globe, and datasets are broken into chunks to facilitate processing. While those outside the LHC collaborations don't typically have access to these resources, it is worth pointing out that the open data from the detectors is already "reconstructed" and "reprocessed" â€” that is, a lot of the gruntwork has already been done to prepare the datasets into a form useful for the final analyses themselves (see *Impact on scientists*, under Section 5). Despite this, analysing the "analysable" data is no easy task. To make life easier, we have provided "derived" or simplified datasets along with the code used to prepare them, so that these smaller datasets can be used for analysis in a classroom or university environment without a large technological overhead.
