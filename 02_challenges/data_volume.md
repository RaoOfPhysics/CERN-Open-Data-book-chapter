## Volume of data

Making data public does not make them any simpler. Since the LHC began delivering collisions in 2010, and as of writing this document (June 2016), the four detectors have recorded several tens of petabytes (PB) of collision data. Performing an analysis on this volume of data involves several stages. First, the data from the collisions, which are in the form of electrical signals from individual channels within each detector, must be pieced together, or reconstructed, to form a coherent picture of the collision itself. Then, analysis software must sift through the collisions to look for signatures associated with predicted physics phenomena or for deviations from the predictions.

The LHC collaborations rely on the Worldwide LHC Computing Grid (or the Grid, for short)\footnote{\url{http://wlcg.web.cern.ch/}} to perform these analysis tasks. The Grid is a network of interconnected computers distributed all over the globe, and datasets are broken into chunks to facilitate processing. While those outside the LHC collaborations don't typically have access to these resources, it is worth pointing out that the research-level open data from the detectors are already "reconstructed" and "reprocessed" (Level 3 data) â€” that is, a lot of the gruntwork has already been done to convert the datasets into a form useful for the final analyses themselves (see also *Impact on scientists*, under Section 5). Despite this, analysing the "analysable" data is no small task and preparing an appropriate dataset, depending on the analysis one wishes to perform, can take up considerable time and computing resources. To make life easier, the Portal provides smaller, simplified datasets -- known as "derived datasets" -- that can be used for analysis in a classroom or university environment without a large technological overhead. The Portal also makes available the code used to prepare these derived datasets.
