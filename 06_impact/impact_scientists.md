## Impact on scientists

When the CMS Collaboration first drafted its open-data policy in 2012, the concept of open research data was entirely new in high-energy physics. The researchers -- correctly -- see the published result as the goal of the research work, and publishing data on their own was seen by some as unnecessary, premature or meaningless. Several concerns were expressed within the collaboration, mainly addressing the potential additional workload for collaboration members in case someone presents contradictory or false results based on the open data from CMS. It was, however, considered that the benefits of the open data, both in terms of preserving their scientific potential and in enabling their use in education, overcome any potential risks.

The high-level open data from CMS -- in the "primary datasets" -- are in the same format that is used within the collaboration for physics analysis. There is no simplification or curation of these data for public release. However, in order to prepare these datasets in the first place, the raw data from the detectors must be "reprocessed" and the same version of the software used in the reprocessing must then be used to perform an analysis on the datasets. The only viable solution for releasing the reprocessed primary datasets with the appropriate software versions was to make them available as part of the CMS Collaboration's legacy data. The legacy data for the 2010 data-collection period are reprocessed in their entirety with a single version of the analysis software. This way, there is no need for additional reprocessing and validation just for a public release, a time- and resource-consuming process. The exact provenance details (such as configuration parameters) can be directly extracted from the CMS internal data-description systems.

That said, finding the human resources to provide adequate instructions and documentation is quite demanding for a collaboration during active data taking, with all the resources focused on collecting and analysing new data. The experience of the first data release from CMS was instructive: even though preparations for the data to be released were started with a fairly short delay after the final reprocessing of the legacy data, and considering that some of the last publications with these data were done just a bit earlier, finding the correct set of instructions and documentation was not easy for many simple reasons. For one, the people in charge had moved to different positions within the collaboration. For another, the documentation had been updated and, although much of it was version-controlled, finding the correct set of instructions corresponding to the exact software release is not as obvious as as it may seem. Having learnt from this, CMS is preparing for smoother data releases in the future by recording all the necessary instructions for data analysis with the data are in active use.

Beginning preparations for releasing open data well in advance has had a major impact on the long-term data- and knowledge-preservation efforts within the CMS Collaboration. Curating the instructions on how to use the data puts the emphasis on the most difficult area in the data preservation: the preservation of knowledge, which at the time of the active use of the data is such an integral part of everyday work that it may not even have been explicitly noted down.

The overwhelmingly positive feedback received following the first data release in 2014 has convinced the CMS Collaboration of undertaking further regular data releases.
